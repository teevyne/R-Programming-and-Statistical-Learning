1.	Describe the null hypotheses to which the p-values given in Table 3.4 correspond. Explain what conclusions you can draw based on these p-values. Your explanation should be phrased in terms of sales, TV, radio, and newspaper, rather than in terms of the coefficients of the linear model.

The null hypotheses associated with table 3.4 are that advertising budgets of “TV”, “radio” or “newspaper” do not have an effect on sales. More precisely H(1)0:β1=0, H(2)0:β2=0 and H(3)0:β3=0. The corresponding p-values are highly significant for “TV” and “radio” and not significant for “newspaper”; so we reject H(1)0 and H(2)0 and we do not reject H(3)0. We may conclude that newspaper advertising budget do not affect sales


2. Carefully explain the differences between the KNN classifier and KNN regression methods.

ANSWER
KNN Regression methods have their codomains in a continuous space. It tries to predict the value of the putput variable by using lical average.

While KN Classifier method has its codomain in a discrete space. It attempts to predict the class to which an output variable belongs by comparing local probabilities.


4. I collect a set of data (n=100 observations) containing a single predictor and a quantitative response. I then fit a linear regression model to the data, as well as a separate cubic regression, i.e. Y=β0+β1X+β2X2+β3X3+ε .

Suppose that the true relationship between X and Y is linear, i.e. Y=β0+β1X+ε. Consider the training residual sum of squares (RSS) for the linear regression, and also the training RSS for the cubic regression. Would we expect one to be lower than the other, would we expect them to be the same, or is there not enough information to tell? Justify your answer.

ANSWER
Without knowing more details about the training data, it is difficult to know which training RSS is lower between linear or cubic. However, as the true relationship between X and Y is linear, we may expect the least squares line to be close to the true regression line, and consequently the RSS for the linear regression may be lower than for the cubic regression.


b.	Answer (a) using test rather than training RSS.

ANSWER
In this case the test RSS depends upon the test data, so we have not enough information to conclude. However, we may assume that polynomial regression will have a higher test RSS as the overfit from training would have more error than the linear regression.

c.	Suppose that the true relationship between X and Y is not linear, but we don’t know how far it is from linear. Consider the training RSS for the linear regression, and also the training RSS for the cubic regression. Would we expect one to be lower than the other, would we expect them to be the same, or is there not enough information to tell? Justify your answer.

ANSWER
Polynomial regression has lower train RSS than the linear fit because of higher flexibility: no matter what the underlying true relationshop is the more flexible model will closer follow points and reduce train RSS.

5.	Consider the fitted values that result from performing linear regression
without an intercept. In this setting, the ith fitted value takes
the form		What is ai?
DONE CHECK FOR SOLUTION ON PAPER


6.	Using (3.4), argue that in the case of simple linear regression, the least squares line always passes through the point (x¯¯¯,y¯¯¯).

ANSWER
The least square line equation is y=β^0+β^1x, so if we substitute x¯¯¯ for x we obtain
y=β^0+β^1x¯¯¯=y¯¯¯−β^1x¯¯¯+β^1x¯¯¯=y¯¯¯.
We may conclude that the least square line passes through the point (x¯¯¯,y¯¯¯). TRUE


7.	It is claimed in the text that in the case of simple linear regression of Y onto X, the R2 statistic (3.17) is equal to the square of the correlation between X and Y (3.18). Prove that this is the case. For simplicity, you may assume that ¯x = ¯y = 0.

ANSWER
DONE ELSEWHERE